{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial Analysis of Internet Access Disparities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will work with real crowdsourced Internet measurement data to identify regional boundaries for sampling Internet latency. The techniques used in this notebook are very similar to those used by the government to figure out boundaries for census tracts, zip codes and neighborhoods in a city.\n",
    "\n",
    "We will be implementing the below clustering pipeline. If you're interested in learning more, we encourage you to check out [this paper](https://arxiv.org/pdf/2405.11138)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pipeline](pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas numpy scikit-learn seaborn h3 geopandas pysal shapely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Spatial Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measurement data for this assignment consists of [Ookla](https://speedtest.net) speed tests conducted in Chicago over the period of January 2022 - May 2023. For simplicity, we will only consider tests conducted by users that subscribed to Comcast, a popular Internet Service Provider (ISP) in the United States. This data typically suffers from a self-selection bias, meaning that we obtain a new measurement only when an Internet user somewhere in Chicago decides to measure their Internet speed. This causes some geographical areas of Chicago to be sparsely sampled in the dataset, causing problems for network operators and regulators to get a real assessment of Internet performance in these areas. \n",
    "\n",
    "To address this sparsity problem, we make an assumption that users conducting tests from nearby regions likely share the same Internet infrastructure, and therefore, experience similar latency. This assumption opens up a world of possibilities. We can now apply spatial interpolation techniques to fill these data gaps by \"borrowing\" data from nearby locations. A widely used algorithm for doing so is called [Inverse Distance Weighting (IDW)](https://en.wikipedia.org/wiki/Inverse_distance_weighting).\n",
    "\n",
    "In this part, we will implement IDW from scratch and use it to estimate latency along a granular grid of points that covers all of Chicago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the dataset (`data/Chicago_speedtests.csv`) into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the locations (longitude and latitude columns) are in degrees. The IDW algorithm typically uses Euclidean distances, which are calculated in units such as miles or meters. Working with degrees is problematic because a distance of 1 degree can represent different distances (in meters or miles) near the equator than on the poles.\n",
    "\n",
    "To solve this problem, we will work with a projected coordinate reference system (CRS) as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe to a GeoDataFrame (Assuming that you loaded the data in a dataframe called df)\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\n",
    "\n",
    "# First set a standard CRS for the GeoDataFrame. This is done because the GeoDataFrame does not have a CRS yet.\n",
    "gdf = gdf.set_crs(epsg=4326)\n",
    "\n",
    "# Convert to EPSG 32616, which is the recommended projection for the Chicago area\n",
    "gdf = gdf.to_crs(epsg=32616)\n",
    "\n",
    "# Re-assign the new coordinates as x and y columns\n",
    "\n",
    "gdf['x'] = gdf.geometry.x\n",
    "gdf['y'] = gdf.geometry.y\n",
    "\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the columns `x` and `y` in `gdf` contain the projected coordinates. We will use these coordinates to implement IDW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Target point generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will generate target points for interpolation, i.e, points where we want latency estimates to be calculated by IDW. \n",
    "\n",
    "The functions provided below help us generate target points at a predefined grid separation. Implement the `generate_grid_points` function by following the hints in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.wkt import loads\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "\n",
    "def load_chicago_boundaries():\n",
    "    bdry = pd.read_csv('data/City_boundaries.csv')\n",
    "    bdry['the_geom'] = bdry['the_geom'].apply(loads)\n",
    "    bdry = gpd.GeoDataFrame(bdry, geometry='the_geom')\n",
    "    bdry = bdry.set_crs(epsg=4326)\n",
    "    bdry = bdry.to_crs(epsg=32616)\n",
    "    return bdry\n",
    "\n",
    "def generate_grid_points(bdry, spacing_in_meters):\n",
    "    # TODO: Get the coordinates for a bounding box (a rectangle) around the city (Hint: Use the `total_bounds` attribute of the GeoDataFrame)\n",
    "\n",
    "    # TODO: Generate grid points within the bounding box at the specified spacing\n",
    "\n",
    "    # Collect the grid points that are actually within the city boundaries (Hint: Use the shapely Point object and geopandas' `contains` method for filtering)\n",
    "    grid_points_within_city = []\n",
    "\n",
    "    # TODO: Perform the filtering here\n",
    "\n",
    "    # Return the grid points\n",
    "    return grid_points_within_city\n",
    "\n",
    "def plot_grid_points(bdry, grid_points):\n",
    "    ax = bdry.plot(color='white', edgecolor='black')\n",
    "    for x, y in grid_points:\n",
    "        ax.plot(x, y, 'ro', markersize=0.5)\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above functions to first generate the grid points at a separation of 200 meters. Then, plot the points on the map of Chicago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. IDW Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDW assigns weights to each nearby data point based on its distance from an unsampled location. It uses these weights to calculate a linear combination of nearby values as an estimate of the target metric at an unsampled location. The relationship between the similarity of nearby data points and their distance is assumed to be inverse in nature.\n",
    "\n",
    "At an unsampled location $(x, y)$, the IDW estimate of latency is given by:\n",
    "\n",
    "$z(x, y) = \\frac{\\sum_{i = 1}^{n} \\frac{z_i}{d_i^p}}{\\sum_{i = 1}^{n} \\frac{1}{d_i^p}}$\n",
    "\n",
    "where:\n",
    "\n",
    "$z(x, y)$: The latency estimate at $(x, y)$.\n",
    "\n",
    "$n$: Number of source points\n",
    "\n",
    "$d_i$: Euclidean distance between $(x, y)$ and the $i^{th}$ source point. Remember that the Euclidean distance between $(x_1, y_1)$ and $(x_2, y_2)$ is given by $\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$.\n",
    "\n",
    "$z_i$: Latency at the $i^{th}$ source point\n",
    "\n",
    "$p$: A power parameter that controls the influence of distance on latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idw(src_x, src_y, target_x, target_y, src_z, power=2):\n",
    "    \"\"\"\n",
    "    Inverse Distance Weighting (IDW) interpolation algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    src_x (array): The x-coordinates of the source points.\n",
    "    src_y (array): The y-coordinates of the source points.\n",
    "    target_x (array): The x-coordinates of the target points.\n",
    "    target_y (array): The y-coordinates of the target points.\n",
    "    values (array): The values associated with the source points.\n",
    "    power (float): The power parameter for the inverse distance weighting.\n",
    "    \n",
    "    Returns:\n",
    "        target_z: An array of interpolated values at the target points.\n",
    "\n",
    "    Hint: Iterate over target points and then use numpy for vectorized operations.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the `idw` function to estimate latency at the grid points generated in 1.2. Use a power parameter of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hexagon Overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have latency estimates at a granular level all over Chicago. Next, we will bin these points into tiny hexagonal cells. Instead of clustering individual points, we will later cluster entire cells that these points fall into. This makes sure that our boundaries are less influenced by latency at individual locations, and more by groups of locations falling in the same area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To bin individual locations into cells, we will use the H3 library. This library was originally created by developers at Uber for taxi demand forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, assign a `hex_id` to each grid location using the H3 library.\n",
    "\n",
    "**Approach:**\n",
    "\n",
    "1. Put the grid locations and latency estimates in a single GeoDataFrame. **[Implemented]**\n",
    "2. Convert the grid locations to spherical coordinates (longitude and latitude, in degrees). **[Implemented]**\n",
    "3. Use an appropriate H3 function to assign a hexagon to each grid location. Check H3 documentation [here](https://h3geo.org/docs). You will notice that the function call requires a resolution value to be specified. Use `resolution=8`. **[TODO]**\n",
    "4. Add the `hex_id` column to the original dataframe and obtain the coordinates for hexagon boundaries. **[Implemented]** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h3\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "def invert_tuples(tuple_of_tuples):\n",
    "    return [(p[1], p[0]) for p in tuple_of_tuples]\n",
    "\n",
    "def hexbin(grid_x, grid_y, grid_z, resolution=8):\n",
    "    \"\"\"\n",
    "    Perform binning of the grid points using the H3 library.\n",
    "\n",
    "    Parameters:\n",
    "    grid_x (array): The x-coordinates of the grid points.\n",
    "    grid_y (array): The y-coordinates of the grid points.\n",
    "    grid_z (array): The values associated with the grid points.\n",
    "    resolution (int): The resolution of the hexagonal grid.\n",
    "\n",
    "    Returns:\n",
    "        hexagons: A list of hexagon IDs.\n",
    "    \"\"\"\n",
    "    # Build a GeoDataFrame with the grid points and latency values\n",
    "    points = pd.DataFrame({'x': grid_x, 'y': grid_y, 'z': grid_z})\n",
    "    points = gpd.GeoDataFrame(points, geometry=gpd.points_from_xy(points.x, points.y))\n",
    "    points = points.set_crs(epsg=32616)\n",
    "    points = points.to_crs(epsg=4326) # Convert to lat-long\n",
    "    x_new = points.geometry.x.values\n",
    "    y_new = points.geometry.y.values\n",
    "    hexagons = []\n",
    "    ############################################################\n",
    "    # TODO: Assign each grid point to a hexagon\n",
    "    ############################################################\n",
    "    # Add the hex_id column in the GeoDataFrame\n",
    "    points['hex_id'] = hexagons\n",
    "    # Get hexagon boundaries in a column\n",
    "    points['hex_boundary'] = points['hex_id'].apply(lambda x: h3.cell_to_boundary(x))\n",
    "    points['hex_boundary'] = points['hex_boundary'].apply(invert_tuples)\n",
    "    points['hex_boundary'] = points['hex_boundary'].apply(lambda x: Polygon(x))\n",
    "    points = gpd.GeoDataFrame(points, geometry='hex_boundary')\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the above function and obtain a GeoDataFrame with assigned `hex_id`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Spatial Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will aggregate latency locally within the hexagon cells to get an assessment of regional performance. We will cluster these aggregates later on to obtain the boundaries. \n",
    "\n",
    "Pick a suitable function for aggregating latency over each cell. Examples could be `mean`, `median`, `standard deviation`, `percentiles`, etc. Also write a justification for why you chose a particular function.\n",
    "\n",
    "You're also welcome to pick more than one function and compare your choices later on.\n",
    "\n",
    "Calculate your aggregate(s) in the below cell. Consider joining the aggregated dataframe back with the `points` dataframe to retain the `hex_boundary` column. This column will later be useful for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block just ensures that we're working with a GeoDataFrame. We will set the geometry column as `hex_boundary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that your aggregated dataframe is called `agg_df`\n",
    "\n",
    "agg_df = gpd.GeoDataFrame(agg_df, geometry='hex_boundary')\n",
    "agg_df = agg_df.set_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Clustering Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below function builds clusters for the choice of two parameters: `n_clusters` and `floor`. `n_clusters` refers to the number of clusters to be formed, and `floor` controls the minimum number of hexagons in a cluster.\n",
    "\n",
    "We have implemented the `cluster_skater` function for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spopt\n",
    "import libpysal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise as skm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def cluster_skater(agg_df, metric, n_clusters, floor):\n",
    "    \"\"\"\n",
    "    Perform clustering of the hexagons using the SKATER algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    agg_df (GeoDataFrame): A GeoDataFrame containing the aggregated data.\n",
    "    metric (str): The column name of the metric to use for clustering.\n",
    "    n_clusters (int): The number of clusters to create.\n",
    "    floor (int): The minimum number of hexagons in each cluster.\n",
    "\n",
    "    Returns:\n",
    "        clusters: A GeoDataFrame containing the clusters.\n",
    "    \"\"\"\n",
    "    w = libpysal.weights.Queen.from_dataframe(agg_df, use_index=False) # Create spatial weights. Used for encoding spatial relationships between hexagons.\n",
    "    spanning_forest_kwds = dict(\n",
    "        dissimilarity=skm.manhattan_distances,\n",
    "        affinity=None,\n",
    "        reduction=np.sum,\n",
    "        center=np.mean,\n",
    "        verbose=False\n",
    "    )\n",
    "    model = spopt.region.Skater(\n",
    "        agg_df,\n",
    "        w,\n",
    "        [metric],\n",
    "        n_clusters=n_clusters,\n",
    "        floor=floor,\n",
    "        trace=False,\n",
    "        islands=\"increase\",\n",
    "        spanning_forest_kwds=spanning_forest_kwds\n",
    "    )\n",
    "    model.solve()\n",
    "    clusters = agg_df.copy()\n",
    "    clusters = clusters.assign(clusters=model.labels_)\n",
    "    clusters['clusters'] = clusters['clusters'].astype(str)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a `floor` of 50 and plot your clusters while raising `n_clusters` from 2 to 11. Beyond what value of `n_clusters` do you observe little or no change in the clusterings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the below function that uses the KMeans algorithm for building clusters. Using the same `n_clusters` as the answer for the previous part, plot and compare the differences between your clusters for KMeans and SKATER. Write your observations in a markdown cell. Think about spatial contiguity and geographic patterns while preparing your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster_kmeans(agg_df, metric, n_clusters):\n",
    "    \"\"\"\n",
    "    Perform clustering of the hexagons using the KMeans algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    agg_df (GeoDataFrame): A GeoDataFrame containing the aggregated data.\n",
    "    metric (str): The column name of the metric to use for clustering.\n",
    "    n_clusters (int): The number of clusters to create.\n",
    "\n",
    "    Returns:\n",
    "        clusters: A GeoDataFrame containing the clusters.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
